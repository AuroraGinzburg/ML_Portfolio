{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74e4110",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "**Get an understanding for which variables are important, view summary statistics, and visualize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e950048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viz the balance of your label column\n",
    "sns.countplot(data = df, x = 'label_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ae7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a histogram of a feature\n",
    "plt.figure(figsize=(10,5))\n",
    "#usually want a hist figsize to be a long rectangle \n",
    "sns.histplot(data = df, x='col_a', bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8675fb",
   "metadata": {},
   "source": [
    "Explore correlation between the **continuous feature variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the correlation\n",
    "df.corr()\n",
    "\n",
    "#Viz with a heatmap\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(df.corr(), cmap='icefire', annot=True)\n",
    "##if a feature is a perfect predictor of a label then it isn't really a feature, it is probably duplicate information\n",
    "\n",
    "#explore correlated continuous features with a scatterplot \n",
    "sns.scatterplot(data = df, x= 'col_a', y='col_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217e695",
   "metadata": {},
   "source": [
    "Explore correlation between the a **continuous** feature and a **categorical** value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz the correlation with a boxplot\n",
    "sns.boxplot(data = df, x= 'categorical_col', y='continuous_col')\n",
    "\n",
    "#calculate the summary statistics behind boxplot\n",
    "df.groupby('categorical_col')['continuous_col'].describe()\n",
    "\n",
    "#Viz- bar plot showing the correlation of the numeric features to a categorical column\n",
    "df.corr()['categorical_col'].sort_values().drop('categorical_col').plot.bar()\n",
    "##Which continuous features have the highest pos or neg correlation with the outcome of the categorical column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9885f35",
   "metadata": {},
   "source": [
    "Explore features in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d37ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return all unique values\n",
    "sorted(df['col_a'].unique())\n",
    "\n",
    "#viz a distribuition of all unique values\n",
    "sns.countplot(data = df, x = 'col_a', palette = 'coolwarm')\n",
    "##differentiate by a categorical value with: hue='categorical_col'\n",
    "##reorder x axis: feature_order = sorted(df['col_a'].unique()) **and**  order = feature_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining a df of some of the columns\n",
    "df[['col_a','col_b', 'col_c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31032a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing a feature\n",
    "someof_col_a = df[(df['col_a']=='PICK ME') | (df['col_a']=='LOOK AT ME')]\n",
    "##someof_col_a is now the callable data to explore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68cce4",
   "metadata": {},
   "source": [
    "## Data PreProcessing\n",
    "**Remove or fill any missing data. Remove unnecessary or repetitive features. Convert categorical string features to dummy variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "#start by looking at distribution of values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column that converts categorical data to 0s and 1s\n",
    "df['new_col'] = df['categorical_col'].map({'Occured':1,'Did Not Occur':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c82ce",
   "metadata": {},
   "source": [
    "Missing Data- see if we should keep, discard, or fill in the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where are you missing data\n",
    "df.isna().sum()\n",
    "\n",
    "#percentage of total missing data\n",
    "100* df.isnull().sum()/len(df)\n",
    "\n",
    "#CAREFUL, if all missing data points are a VERY small percentage of the df, you can drop them\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7681d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at total unique values per column with some missing data, then see value counts for those\n",
    "df['col_missing_data'].nunique()\n",
    "df['col_missing_data'].value_counts()\n",
    "\n",
    "#Viz a countplot of a column with missing data\n",
    "desiredorder= sorted(df['col_missing_data'].dropna().unique())\n",
    "sns.countplot(x='col_missing_data',data=df,order=desiredorder)\n",
    "##if the sorted order isn't how you want to plot it, rewrite the order in a list as a variable \n",
    "##can add in countplot arg: hue= 'label_col'\n",
    "\n",
    "#Examine ratio of label outcome per col_missing_data category\n",
    "label_0 = df[df['label_col']==\"zero\"].groupby(\"col_missing_data\").count()['label_col']\n",
    "label_1 = df[df['label_col']==\"one\"].groupby(\"col_missing_data\").count()['label_col']\n",
    "label_ratio = label_0/label_1\n",
    "label_ratio\n",
    "label_ratio.plot(kind='bar')\n",
    "##Is the label outcome similar across categories? If not maybe drop 'col_missing_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92512665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete a feature with too much missing data and unique values\n",
    "df.drop('col_missing_data', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bcfb4",
   "metadata": {},
   "source": [
    "Duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine values for columns that may be repeated information\n",
    "df['col_a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c11c2c",
   "metadata": {},
   "source": [
    "Filling in missing data- Many ways to deal with missing data. We could attempt to build a simple model to fill it in, such as a linear model, we could just fill it in based on the mean of the other columns, or you could even bin the columns into categories and then set NaN as its own category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a value_counts of a column\n",
    "df['col_c_missing_data'].value_counts()\n",
    "\n",
    "#Review the other columsn to see which most highly correlates to col_c_missing_data\n",
    "print(\"Correlation with the col_c_missing_data\")\n",
    "df.corr()['col_c_missing_data'].sort_values()\n",
    "\n",
    "#If the feature that correlates most with col_c_missing_data makes sense, we can try a fillna() approach.\n",
    "#Group the df by the correlated feature and calculate the mean value for the col_c_missing_data per correlated feature entry. \n",
    "print(\"Mean of col_c_missing_data column per correlated_feat\")\n",
    "correlated_feat_avg = df.groupby('correlated_feat').mean()['col_c_missing_data']\n",
    "\n",
    "#Fill in the missing col_c_missing_data values based on their correlated_feat value. \n",
    "#Fill in that missing value with the mean value corresponding to its correlated_feat value from the Series we created above.\n",
    "correlated_feat_avg[2.0]\n",
    "##demonstrating that we can grab the value by entering the correlated_feat value as the slice **2.0 may need to change based on values from above Series**\n",
    "\n",
    "\n",
    "def fill_col_c(correlated_feat,col_c_missing_data):\n",
    "    '''\n",
    "    Accepts both arg values for the row.\n",
    "    Checks if the col_c_missing_data is NaN , if so, it returns the avg correlated_feat value\n",
    "    for the corresponding col_c_missing_data value for that row.\n",
    "    \n",
    "    correlated_feat_avg here should be a Series or dictionary containing the mapping of the\n",
    "    groupby averages of col_c_missing_data per correlated_feat values.\n",
    "    '''\n",
    "    if np.isnan(col_c_missing_data):\n",
    "        return correlated_feat_avg[correlated_feat]\n",
    "    else:\n",
    "        return col_c_missing_data\n",
    "    \n",
    "df['col_c_missing_data'] = df.apply(lambda x: fill_col_c(x['correlated_feat'], x['col_c_missing_data']), axis=1)\n",
    "**order of args in defined func needs to match order they are filled in the apply method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ebc6a",
   "metadata": {},
   "source": [
    "**Categorical Variables and Dummy Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4af51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the columns that are currently non-numeric\n",
    "df.select_dtypes(['object']).columns\n",
    "##may want to do something with the string variables\n",
    "\n",
    "#Turning strings to variables\n",
    "df['col_d'] = df['col_d'].map({' 36 months': 36, ' 60 months': 60})\n",
    "\n",
    "#Reducing the nunique for a string column\n",
    "df['col_j'].value_counts()\n",
    "df['col_j']=df['col_j'].replace(['dumbvalue1', 'dumbvalue2'], 'lessdumbvalue')\n",
    "##'dumbvalue1', 'dumbvalue2','lessdumbvalue' are already values in value_counts and it makes logical sense to change them all to lessdumbvalue\n",
    "\n",
    "#Feature enginer a zip code column from an address\n",
    "df['zip_code'] = df['address'].apply(lambda address:address[-5:])\n",
    "##here the zip code is the last 5 values in the address\n",
    "##may need to add int(address[-5:])\n",
    "\n",
    "#ensure none of the features are causing data leakage (giving away the label outcome)\n",
    "df = df.drop('cheater_col',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concert a string to dummy variables- remember to add drop_first=True to avoid duplicate info in the df\n",
    "col_e_dummies = pd.get_dummies(df['col_e'],drop_first=True)\n",
    "\n",
    "#concatenate these new columns to the original dataframe and drop the original str column\n",
    "df = pd.concat([df.drop('col_e',axis=1),col_e_dummies],axis=1)\n",
    "\n",
    "#do dummy variable creation, concat and column dropping for multiple features at once\n",
    "dummies = pd.get_dummies(df[['col_x', 'col_y','col_z','col_w' ]],drop_first=True)\n",
    "df = df.drop(['col_x', 'col_y','col_z','col_w' ],axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca6a7e",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768763ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('lavel_col',axis=1).values\n",
    "y = df['label_col'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22300d8",
   "metadata": {},
   "source": [
    "Grabbing a Sample for Saving Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b27cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "small_df = df.sample(frac=0.1,random_state=101)\n",
    "print(len(small_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dce107",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "#Ensure the length of the df and the num of columns is what you expect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2b36f",
   "metadata": {},
   "source": [
    "## Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#We don't want data leakge from the test set so we only fit on the X_train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd2ace",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8669ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(78, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(39, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(19, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile (optimizer = 'adam', loss= 'binary_crossentropy')\n",
    "\n",
    "#This is for a categorical label and a 78 column X.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc749e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163634ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss', mode='min', verbose = 1, patience = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs = 600,\n",
    "         batch_size = 256,\n",
    "         validation_data = (X_test, y_test), verbose =1,\n",
    "         callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440648f",
   "metadata": {},
   "source": [
    "Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('model_name.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721883fb",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bfec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6874e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeda10f",
   "metadata": {},
   "source": [
    "## Use Model to Predict an Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_ind = random.randint(0,len(df))\n",
    "\n",
    "new_item = df.drop('label_col',axis=1).iloc[random_ind]\n",
    "new_item\n",
    "\n",
    "model.predict_classes(new_item.values.reshape(-1,78))\n",
    "##78 is the column num from x_train.shape\n",
    "\n",
    "#check true label\n",
    "df.iloc[random_ind]['label_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16015c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\18053\\\\Desktop\\\\Reference Jupyter Notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcb1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
